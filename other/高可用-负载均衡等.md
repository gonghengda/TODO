## 什么是负载均衡

负载均衡是许多协同工作资源(通常是计算机)的分配策略。它们通常用于提高容量和可靠性。  

为了便于讨论负载均衡，对于服务扩展假我设两点：  
 - 我可以运行任意数量实例  
 - 任何请求可以到达任意实例  

第一个假设表明服务是无状态的（或者像Redis集群一样可以共享状态）。第二个假设实际中并不是必须的（比如粘性负载均衡），本文只为便于讨论  

#### 负载均衡技术：

1. 应用层(OSI第七层) 负载均衡(HTTP,HTTPS,WS)  
2. 传输层(OSI第四层) 负载均衡(TCP,UDP)  
3. 网络层(OSI第三层) 负载均衡  
4. DNS负载均衡  
5. 多个子域手动负载均衡  
6. 任播(Anycast)  

ps: 
  OSI(Open System Interconnection): 开放式系统互联   
  TCP(Transmission Control Protocol 传输控制协议):是一种面向连接的，可靠的，基于字节流的传输层通信协议。  
  UDP(User Datagram Protocol 用户数据报协议)：是一种无连接的传输层协议，提供面向事务的简单不可靠信息传送服务.  

最后还有点其他知识:  

 - 延迟和吞吐量  
 - 服务器直接返回  

这些技术大致按照[网站流量增大时需求的操作步骤]排序。比如，应用层的负载均衡是首先要做的事（远远早于任播）。前三条技术提高  
吞吐量和可用性，但存在单点故障。  其余的三条技术在提高吞吐量的同时避免了单点故障  


## 服务

假设我们正在构建大规模的服务：  

![p1](../负载均衡/p1.jpeg)  

这个系统并不能处理大量通信，并且如果它宕机，整个应用就停了。  

**比喻**  

你走向商店唯一一条结账的队伍。  
你购买你的商品，如果那里没有收银员，你无法完成购买。  


## 应用层(OSI第七层)负载均衡

为了承载更大的通信量，首先用到应用层负载均衡。OSI第七层.它包括 HTTP,HTTPS,WebSockets.一款非常流行久经考验的应用层负载均衡器  
就是 Nginx.  
如下图：  

![p1](../负载均衡/p2.jpeg)

ps: 通过这种技术，我们能负载均衡数十或上百服务实例。上面图片只展示两个作为例子.  

**比喻**  

商店员工引领你到一个特别的（有收银员的）结账队伍  
你购买你的商品  

**工具**   

 - Nginx  
 - HAproxy  

ps: HAProxy 是一个使用C语言编写的开发源码软件，提供高可用性，负载均衡  
其适用于特别大的web站点。HAProxy运行在当前的硬件上，完全可以支持数以万计的并发连接。  
它的运行模式使得它可以很简单安全的整合进你当前架构中，同时保护你的web服务器不被暴露到网络上  
    HAProxy实现了一种事件驱动，单一进程模型，此模型支持非常大的并发连接数。  
事件驱动模型因为在有更好的资源和时间管理的用户空间(User-Space) 实现所有这些任务，  
所以没有这些问题。此模型的弊端是，在多核系统上，这些程序通常扩展性较差。   
这就是为什么他们必须进行优化以使每个CPU时间片(Cycle)做更多的工作  
    包括 GitHub、Bitbucket[3]、Stack Overflow[4]、Reddit、Tumblr、Twitter[5][6]和 Tuenti[7]在内的知名网站，  
及亚马逊网络服务系统都使用了HAProxy。  

**注意**  

 - 我们要在这里停止使用SSL(分发时不用SSL)  
 
## 传输层(OSI第四层) 负载均衡(TCP,UDP)

上一条技术帮助我们承载大量通信，但如果我们需要承载更大量的通信量，传输层负载均衡非常有用。传输层时OSI第四层  
包括TCP和UDP.流行的传输层负载均衡器有HAProxy(这个也用于应用层负载均衡)和IPVS.  

ps: IPVS基本上是一种高效的Layer-4交换机，它提供负载平衡的功能;  
工作原理：当一个TCP连接的初始SYN报文到达时，IPVS就选择一台服务器，将报文转发给它。此后通过查发报文的IP和TCP报文头地址，  
保证此连接的后继报文被转发到相同的服务器。这样，IPVS无法检查到请求的内容再选择服务器，这就要求后端的服务器组是提供相同的服务  
不管请求被送到哪一台服务器，返回结果都应该是一样的。但是在有一些应用中后端>的服务器可能功能不一，有的是提供HTML文档的Web服务  
器，有的是提供图片的Web服务器，有的是提供CGI的Web服务器。这时，就需要基于内容请求分发 (Content-Based Request Distribution)   
，同时基于内容请求分发可以提高后端服务器上访问的局部性。  

运作如图:  

![p1](../负载均衡/p3.jpeg)

 应用层负载均衡+传输层负载均衡能处理大多数情况下的通信量。然而我们我们仍要担心可用性。单点故障有可能出现在传输层的负载均衡器。  
 我们在下面一节的 DNS 负载均衡解决这个问题。  

**比喻**  


根据客户会员卡卡号有不同结账区域。举个例子，如果你的会员卡卡号是偶数，去电器区附近的结账台，否则就去食品区附近的结账台。  

一旦你到达正确的结账区域，商店员工引领你到一个特别的结账队伍  

你购买你的商品  

## 网络层(OSI第三层) 负载均衡

如果我们要继续扩展，我们需要增加网络层负载均衡。这比上面两条技术更复杂。网络层是 OSI 第三层，包括 IPv4 和 IPv6。  
下面是网络层负载均衡：  

![p1](../负载均衡/p4.jpeg)

为了搞清楚它如何工作，我们需要一点等价路由的知识(ECMP).当有多条等价链路到达相同地址时，我们使用等价路由。  
简单来说，它允许路由或交换机通过不同链接发送数据包（支持高吞吐量），最终到达同一地址。  

我们可以利用这里点来实现网络层负载均衡，因为在我们看来，每个传输层负载均衡器时相同的。这意味着我们把从网络层负载均衡器到  
传输层负载均衡器的链接看做相同目的地的链路。如果我们把所有负载均衡器绑定到相同IP地址，我们可以使用等价路由在传输层负载均衡器  
之间分配通信。  

ps: 
 ECMP:ECMP存在多条不同链路到达同一目的地址的网络环境中，是一种网络术语。  
 ECMP存在多条不同链路到达同一目的地址的网络环境中，如果使用传统的路由技术，发往该目的地址的数据包只能利用其中的一条链路，  
 其它链路处于备份状态或无效状态，并且在动态路由环境下相互的切换需要一定时间，而等值多路径路由协议可以在该网络环境下同时  
 使用多条链路，不仅增加了传输带宽，并且可以无时延无丢包地备份失效链路的数据传输.  
     但是实际情况是，各路径的带宽、时延和可靠性等不一样，把Cost认可成一样，不能很好地利用带宽，尤其在路径间差异大时，  
 效果会非常不理想例 如，路由器两个出口，两路径，一个带宽是100M，一个是2M，如果部署是ECMP，则网络总带宽只能达到4M的利用率。

 **比喻**  


街对面有两家彼此分开却又一模一样的商店，你去哪一家完全取决于你的习惯。  

一旦你到达了商店，根据客户会员卡卡号有不同结账区域。举个例子，如果你的会员卡卡号是偶数，去电器区附近的结账台，否则就去食品区附近的结账台。  

一旦你到达正确的结账区域，商店员工引领你到一个特别的结账队伍  

你购买你的商品  

 **工具**  

  - 通常在机柜里交换机内部的硬件中处理。  
  - 除非你的服务规模相当大或有自己的硬件，否则你不需要它。  

## DNS负载均衡

DNS是将名称转换为IP地址的系统。举个例子，它可以把example.com转换为93.184.216.34 它当然也可以返回多个地址，想下面一样：  

![p1](../负载均衡/p5.jpeg)

如果返回了多个IP ,客户端通常会使用第一个可用的地址（然而一些应用只看第一个返回的ip）.  

目前有很多DNS负载均衡技术，比如GeoDNS和轮询调度(round-robin).GeoDNS基于不同请求者返回不同响应。这让我们可以将客户端路由到其最近  
的服务器数据中心。轮询调度会循环所有可用的IP地址，对于每个响应会返回不同的IP.如果多个IP可用，这两种技术仅仅改变响应里的IP顺序  

ps:
  GeoDNS是一个为BIND写的40行的小程序，可以让DNS解析的时候考虑地域因素——让用户能够访问离他地域最近的Web服务器。  

  round-robin: 算法就是以轮询的方式依次将请求调度不同的服务器，即每次调度执行i = (i + 1) mod n，  
  并选出第i台服务器。算法的优点是其简洁性，它无需记录当前所有连接的状态，所以它是一种无状态调度.  

DNS负载均衡如图:  


![p1](../负载均衡/p6.jpeg)

这个例子中，不同的用户被路由到不同的服务集群（随记或基于地理位置）  

现在这里不再有单点故障的可能性（假设有多台DNS服务器）。为了进一步提高可靠性，我们可以在不同数据中心运行多个服务集群.  

 **比喻**  

 你在网上查询购物中心，返回的列表把最近的购物中心放在第一个。你查看通往每个购物中心的路，然后选择列表中第一个营业的购物中心。  

  街对面有两家彼此分开却又一模一样的商店，你去哪一家完全取决于你的习惯。  

  一旦你到达了商店，根据客户会员卡卡号有不同结账区域。举个例子，如果你的会员卡卡号是偶数，去电器区附近的结账台，否则就去食品区附近的结账台。  
  
  一旦你到达正确的结账区域，商店员工引领你到一个特别的结账队伍  

  你购买你的商品  

## 任播（Anycast）

大多数网路使用单播。这本质上意味着每台计算机拥有独一无二的 IP 地址。有另一种称为任播的理论。  
通过任播，一些机器可以使用相同的 IP地址和路由，并把请求发送到最近的一台机器。  
我们可以把这种技术和上面所讲的技术结合起来，构建出高可靠性和可用性，  
能承载巨大通信量的系统。

任播根本上来说是允许互联网为我们处理部分负载均衡。  

 **比喻**  

你告诉别人你打算去商店，他们把你带到最近的位置。  

街对面有两家彼此分开却又一模一样的商店，你去哪一家完全取决于你的习惯。  

一旦你到达了商店，根据客户会员卡卡号有不同结账区域。举个例子，如果你的会员卡卡号是偶数，去电器区附近的结账台，否则就去食品区附近的结账台。  

一旦你到达正确的结账区域，商店员工引领你到一个特别的结账队伍  

你购买你的商品  

## 杂项

#### 延迟和吞吐量

顺便提的是，这些技术也可以提升低延迟服务的吞吐量。增加服务数量而不是让每个服务处理更多的通信，这样我们就可以得到低延迟，高吞吐量的系统  

#### 服务器直接返回

在传统负载均衡系统中，请求穿过负载均衡的所有层级，响应也同样穿过她们。降低负载均衡通信量的一个优化点就是服务器直接返回。  
这意味着服务端的响应不通过负载均衡。如果服务端的响应十分巨大，这点尤其有用。 